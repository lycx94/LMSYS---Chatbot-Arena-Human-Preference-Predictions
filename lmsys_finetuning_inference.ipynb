{"cells":[{"cell_type":"markdown","metadata":{},"source":["#  Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git \n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets"]},{"cell_type":"markdown","metadata":{},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datasets import Dataset\n","\n","import torch\n","\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments\n","from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training, PeftConfig, PeftModel\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, log_loss"]},{"cell_type":"markdown","metadata":{},"source":["# For reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.backends.cuda.enable_mem_efficient_sdp(False) # faster comput, more ram usage\n","torch.backends.cuda.enable_flash_sdp(False) # accuracy over speed on floating-point arithmetic\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n","display(df_train.head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Concatenate strings in list\n","def process(input_str):\n","    stripped_str = input_str.strip('[]')\n","    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n","    return  ' '.join(sentences)\n","\n","df_train.loc[:, 'prompt'] = df_train['prompt'].apply(process)\n","df_train.loc[:, 'response_a'] = df_train['response_a'].apply(process)\n","df_train.loc[:, 'response_b'] = df_train['response_b'].apply(process)\n","\n","display(df_train.head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Prepare text for model\n","df_train['text'] = 'You are an expert at predicting user preference. \\nUser prompt: ' + df_train['prompt'] +  '\\n\\nModel A :\\n' + df_train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + df_train['response_b']\n","print(df_train['text'][0])"]},{"cell_type":"markdown","metadata":{},"source":["# Parameter setting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_id = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1' # model to finetune\n","from_ckpt = True  # load from ckeckpoint or not\n","peft_model_id = \"/kaggle/input/lmsus-mistral-model/lmsys/model\" # adapter\n","peft_tokenizer_id = \"/kaggle/input/lmsus-mistral-model/lmsys/tokenizer\"\n","\n","MAX_LENGTH = 512 \n","NUM_TARGETS = 3"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize the text data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token_id = 0\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def tokenize_texts(texts, tokenizer, max_length=MAX_LENGTH):\n","    tokens = tokenizer(texts.tolist(), padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n","    return tokens['input_ids'], tokens['attention_mask']\n","\n","\n","from torch.utils.data import Dataset\n","class TextDataset(Dataset):\n","    def __init__(self, input_ids, attention_masks, labels=None):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_masks[idx]\n","        }\n","        if self.labels is not None:\n","            item['labels'] = self.labels[idx]\n","        return item\n","    \n","\n","# Target columns to label\n","target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","y = df_train[target_columns].idxmax(axis=1)\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Split the data into training and validation sets\n","x_train, x_val, y_train, y_val = train_test_split(df_train['text'], y_encoded, test_size=0.1, random_state=42)\n","print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n","\n","# Tokenize the texts\n","input_ids_train, attention_masks_train = tokenize_texts(x_train, tokenizer)\n","input_ids_val, attention_masks_val = tokenize_texts(x_val, tokenizer)\n","    \n","# Create the datasets\n","train_dataset = TextDataset(input_ids_train, attention_masks_train, torch.tensor(y_train))\n","val_dataset = TextDataset(input_ids_val, attention_masks_val, torch.tensor(y_val))"]},{"cell_type":"markdown","metadata":{},"source":["# Load model in 4-bit using BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, # quantize the model to 4-bits when you load it\n","    bnb_4bit_quant_type=\"nf4\", # use a special 4-bit data type for weights initialized from a normal distribution\n","    bnb_4bit_use_double_quant=True, # use a nested quantization scheme to quantize the already quantized weights\n","    bnb_4bit_compute_dtype=torch.bfloat16, # use bfloat16 for faster computation\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_id,     \n","    num_labels=NUM_TARGETS,\n","    quantization_config=bnb_config, \n","    device_map={\"\":0}\n",")\n","\n","\n","if from_ckpt:\n","    print(\"Load from checkpoint...\")\n","    config = PeftConfig.from_pretrained(peft_model_id)\n","    model = PeftModel.from_pretrained(model, peft_model_id)\n","    \n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)\n","\n","config = LoraConfig(\n","    r=8, \n","    lora_alpha=32,  \n","    target_modules=[\"o_proj\", \"v_proj\"], \n","    lora_dropout=0.05, \n","    bias=\"none\", \n","    task_type=\"SEQ_CLS\"\n",")\n","\n","\n","model = get_peft_model(model, config)\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["output_dir = \"lmsys\"\n","ckpt_dir = os.path.join(output_dir, \"checkpoints\")\n","log_file = os.path.join(output_dir, \"training_log.txt\")\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","training_args = TrainingArguments(\n","    output_dir=ckpt_dir,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=4,\n","    save_strategy=\"steps\",\n","    save_steps=3,\n","    save_total_limit=3,\n","    logging_steps = 2,\n","    metric_for_best_model='accuracy',\n","    eval_strategy=\"steps\",\n","    report_to = 'none',\n","    fp16=True,  # Enable mixed precision training\n","    gradient_checkpointing=True  # Enable gradient checkpointing\n",")\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    acc = accuracy_score(labels, preds)\n","    return {'accuracy': acc}\n","\n","from datetime import datetime\n","from transformers import TrainerCallback\n","class LoggingCallback(TrainerCallback):\n","    def __init__(self, log_file):\n","        self.log_file = log_file\n","    \n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            timestamp = datetime.now().strftime('%Y/%m/%d %H-%M-%S')\n","            with open(self.log_file, \"a\") as f:\n","                f.write(f\"{timestamp} - Step: {state.global_step} - {logs}\\n\")\n","            print(f\"{timestamp} - Step: {state.global_step} - {logs}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fix: ValueError: Cannot handle batch sizes > 1 if no padding token is defined.\n","model.config.pad_token_id=50256\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,    \n","    compute_metrics=compute_metrics,\n","    callbacks=[LoggingCallback(log_file)]\n",")\n","\n","model.config.use_cache = False\n","\n","# train model\n","trainer.train()\n","\n","# save model\n","tokenizer.save_pretrained(os.path.join(output_dir, \"tokenizer\"))\n","model.save_pretrained(os.path.join(output_dir, \"model\"))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
