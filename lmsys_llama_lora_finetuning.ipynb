{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef73c98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:15:39.864555Z",
     "iopub.status.busy": "2024-06-04T08:15:39.864226Z",
     "iopub.status.idle": "2024-06-04T08:15:40.414810Z",
     "shell.execute_reply": "2024-06-04T08:15:40.413903Z"
    },
    "papermill": {
     "duration": 0.562166,
     "end_time": "2024-06-04T08:15:40.417041",
     "exception": false,
     "start_time": "2024-06-04T08:15:39.854875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching model 'metaresearch/llama-3/transformers/8b-chat-hf' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /kaggle/input/llama-3/transformers/8b-chat-hf/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"metaresearch/llama-3/transformers/8b-chat-hf\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb64f9a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-04T08:15:40.434193Z",
     "iopub.status.busy": "2024-06-04T08:15:40.433875Z",
     "iopub.status.idle": "2024-06-04T08:15:41.139465Z",
     "shell.execute_reply": "2024-06-04T08:15:41.138603Z"
    },
    "papermill": {
     "duration": 0.716613,
     "end_time": "2024-06-04T08:15:41.141805",
     "exception": false,
     "start_time": "2024-06-04T08:15:40.425192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\n",
      "/kaggle/input/lmsys-chatbot-arena/train.csv\n",
      "/kaggle/input/lmsys-chatbot-arena/test.csv\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n",
      "/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67e410e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T08:15:41.158634Z",
     "iopub.status.busy": "2024-06-04T08:15:41.158228Z",
     "iopub.status.idle": "2024-06-04T08:15:59.472167Z",
     "shell.execute_reply": "2024-06-04T08:15:59.470969Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 18.32504,
     "end_time": "2024-06-04T08:15:59.474670",
     "exception": false,
     "start_time": "2024-06-04T08:15:41.149630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.1)\r\n",
      "Collecting peft\r\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mInstalling collected packages: bitsandbytes, peft\r\n",
      "Successfully installed bitsandbytes-0.43.1 peft-0.11.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate bitsandbytes transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99446002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:15:59.501084Z",
     "iopub.status.busy": "2024-06-04T08:15:59.500379Z",
     "iopub.status.idle": "2024-06-04T08:16:17.889908Z",
     "shell.execute_reply": "2024-06-04T08:16:17.889078Z"
    },
    "papermill": {
     "duration": 18.405169,
     "end_time": "2024-06-04T08:16:17.892193",
     "exception": false,
     "start_time": "2024-06-04T08:15:59.487024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:16:08.056982: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-04 08:16:08.057095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-04 08:16:08.159909: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08180ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:17.917947Z",
     "iopub.status.busy": "2024-06-04T08:16:17.917065Z",
     "iopub.status.idle": "2024-06-04T08:16:17.921554Z",
     "shell.execute_reply": "2024-06-04T08:16:17.920732Z"
    },
    "papermill": {
     "duration": 0.019131,
     "end_time": "2024-06-04T08:16:17.923546",
     "exception": false,
     "start_time": "2024-06-04T08:16:17.904415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5c59a",
   "metadata": {
    "papermill": {
     "duration": 0.011626,
     "end_time": "2024-06-04T08:16:17.947998",
     "exception": false,
     "start_time": "2024-06-04T08:16:17.936372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5cb4b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:17.972743Z",
     "iopub.status.busy": "2024-06-04T08:16:17.972442Z",
     "iopub.status.idle": "2024-06-04T08:16:17.979689Z",
     "shell.execute_reply": "2024-06-04T08:16:17.978863Z"
    },
    "papermill": {
     "duration": 0.021818,
     "end_time": "2024-06-04T08:16:17.981548",
     "exception": false,
     "start_time": "2024-06-04T08:16:17.959730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1c548",
   "metadata": {
    "papermill": {
     "duration": 0.011591,
     "end_time": "2024-06-04T08:16:18.004821",
     "exception": false,
     "start_time": "2024-06-04T08:16:17.993230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589cc37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:18.029657Z",
     "iopub.status.busy": "2024-06-04T08:16:18.029398Z",
     "iopub.status.idle": "2024-06-04T08:16:21.535299Z",
     "shell.execute_reply": "2024-06-04T08:16:21.534311Z"
    },
    "papermill": {
     "duration": 3.520859,
     "end_time": "2024-06-04T08:16:21.537725",
     "exception": false,
     "start_time": "2024-06-04T08:16:18.016866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57477, 9)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "df_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# df_train = df_train[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398068a8",
   "metadata": {
    "papermill": {
     "duration": 0.011566,
     "end_time": "2024-06-04T08:16:21.561490",
     "exception": false,
     "start_time": "2024-06-04T08:16:21.549924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637e6468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:21.586619Z",
     "iopub.status.busy": "2024-06-04T08:16:21.586302Z",
     "iopub.status.idle": "2024-06-04T08:16:22.200583Z",
     "shell.execute_reply": "2024-06-04T08:16:22.199657Z"
    },
    "papermill": {
     "duration": 0.629048,
     "end_time": "2024-06-04T08:16:22.202589",
     "exception": false,
     "start_time": "2024-06-04T08:16:21.573541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model_path = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012069c0",
   "metadata": {
    "papermill": {
     "duration": 0.011793,
     "end_time": "2024-06-04T08:16:22.226699",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.214906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60a763d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.251731Z",
     "iopub.status.busy": "2024-06-04T08:16:22.251447Z",
     "iopub.status.idle": "2024-06-04T08:16:22.256328Z",
     "shell.execute_reply": "2024-06-04T08:16:22.255582Z"
    },
    "papermill": {
     "duration": 0.019456,
     "end_time": "2024-06-04T08:16:22.258081",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.238625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define MAX_LENGTH\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_texts(texts, tokenizer, max_length=MAX_LENGTH):\n",
    "    tokens = tokenizer(texts.tolist(), padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebbe2a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.283146Z",
     "iopub.status.busy": "2024-06-04T08:16:22.282859Z",
     "iopub.status.idle": "2024-06-04T08:16:22.289230Z",
     "shell.execute_reply": "2024-06-04T08:16:22.288386Z"
    },
    "papermill": {
     "duration": 0.021094,
     "end_time": "2024-06-04T08:16:22.291108",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.270014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create pytorch dataset for transformers Trainer\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879cb636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.316966Z",
     "iopub.status.busy": "2024-06-04T08:16:22.316288Z",
     "iopub.status.idle": "2024-06-04T08:16:22.320344Z",
     "shell.execute_reply": "2024-06-04T08:16:22.319514Z"
    },
    "papermill": {
     "duration": 0.018896,
     "end_time": "2024-06-04T08:16:22.322290",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.303394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01cd69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.347536Z",
     "iopub.status.busy": "2024-06-04T08:16:22.347276Z",
     "iopub.status.idle": "2024-06-04T08:16:22.477710Z",
     "shell.execute_reply": "2024-06-04T08:16:22.476572Z"
    },
    "papermill": {
     "duration": 0.145378,
     "end_time": "2024-06-04T08:16:22.479717",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.334339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90,) (10,) (90,) (10,)\n"
     ]
    }
   ],
   "source": [
    "columns_to_vectorize = ['prompt', 'response_a', 'response_b']\n",
    "df_train['text'] = df_train[columns_to_vectorize].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "df_test['text'] = df_test[columns_to_vectorize].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "y = df_train[target_columns].idxmax(axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_train['text'], y_encoded, test_size=0.1, random_state=42)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "# Tokenize the texts\n",
    "input_ids_train, attention_masks_train = tokenize_texts(x_train, tokenizer)\n",
    "input_ids_val, attention_masks_val = tokenize_texts(x_val, tokenizer)\n",
    "input_ids_test, attention_masks_test = tokenize_texts(df_test['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2542fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.505122Z",
     "iopub.status.busy": "2024-06-04T08:16:22.504802Z",
     "iopub.status.idle": "2024-06-04T08:16:22.521795Z",
     "shell.execute_reply": "2024-06-04T08:16:22.521119Z"
    },
    "papermill": {
     "duration": 0.031828,
     "end_time": "2024-06-04T08:16:22.523655",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.491827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = TextDataset(input_ids_train, attention_masks_train, torch.tensor(y_train))\n",
    "val_dataset = TextDataset(input_ids_val, attention_masks_val, torch.tensor(y_val))\n",
    "test_dataset = TextDataset(input_ids_test, attention_masks_test)\n",
    "\n",
    "## Create the dataloaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c97d1",
   "metadata": {
    "papermill": {
     "duration": 0.011838,
     "end_time": "2024-06-04T08:16:22.547386",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.535548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b21d5a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:16:22.572456Z",
     "iopub.status.busy": "2024-06-04T08:16:22.572188Z",
     "iopub.status.idle": "2024-06-04T08:18:17.428078Z",
     "shell.execute_reply": "2024-06-04T08:18:17.427261Z"
    },
    "papermill": {
     "duration": 114.871066,
     "end_time": "2024-06-04T08:18:17.430377",
     "exception": false,
     "start_time": "2024-06-04T08:16:22.559311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d68b5504d842619170dcfe109df345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3/transformers/8b-chat-hf/1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from_ckpt = False  \n",
    "output_dir = \"lmsys\"\n",
    "ckpt_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "ckpt_name = \"llama3\"\n",
    "\n",
    "base_model_path = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "cache_dir = \"./cache\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['o_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_path,\n",
    "    num_labels=len(target_columns),\n",
    "    quantization_config=nf4_config,\n",
    "    low_cpu_mem_usage = True\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if from_ckpt:\n",
    "    model = PeftModel.from_pretrained(base_model, ckpt_name)\n",
    "else:\n",
    "    model = base_model\n",
    "\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05eb53fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:18:17.456730Z",
     "iopub.status.busy": "2024-06-04T08:18:17.456421Z",
     "iopub.status.idle": "2024-06-04T08:18:17.464900Z",
     "shell.execute_reply": "2024-06-04T08:18:17.463822Z"
    },
    "papermill": {
     "duration": 0.023792,
     "end_time": "2024-06-04T08:18:17.466792",
     "exception": false,
     "start_time": "2024-06-04T08:18:17.443000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 536,576 || all params: 7,505,473,536 || trainable%: 0.0071\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "806b1690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:18:17.493215Z",
     "iopub.status.busy": "2024-06-04T08:18:17.492915Z",
     "iopub.status.idle": "2024-06-04T08:18:17.518426Z",
     "shell.execute_reply": "2024-06-04T08:18:17.517726Z"
    },
    "papermill": {
     "duration": 0.041181,
     "end_time": "2024-06-04T08:18:17.520419",
     "exception": false,
     "start_time": "2024-06-04T08:18:17.479238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=ckpt_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    save_total_limit=5,\n",
    "    metric_for_best_model='accuracy',\n",
    "    report_to = 'none',\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    gradient_checkpointing=True  # Enable gradient checkpointing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d93658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:18:17.546281Z",
     "iopub.status.busy": "2024-06-04T08:18:17.545973Z",
     "iopub.status.idle": "2024-06-04T08:18:17.550634Z",
     "shell.execute_reply": "2024-06-04T08:18:17.549797Z"
    },
    "papermill": {
     "duration": 0.019521,
     "end_time": "2024-06-04T08:18:17.552468",
     "exception": false,
     "start_time": "2024-06-04T08:18:17.532947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e8af8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:18:17.578372Z",
     "iopub.status.busy": "2024-06-04T08:18:17.577400Z",
     "iopub.status.idle": "2024-06-04T08:18:17.877677Z",
     "shell.execute_reply": "2024-06-04T08:18:17.876791Z"
    },
    "papermill": {
     "duration": 0.315055,
     "end_time": "2024-06-04T08:18:17.879579",
     "exception": false,
     "start_time": "2024-06-04T08:18:17.564524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dec12c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:18:17.905634Z",
     "iopub.status.busy": "2024-06-04T08:18:17.905333Z",
     "iopub.status.idle": "2024-06-04T08:22:26.916059Z",
     "shell.execute_reply": "2024-06-04T08:22:26.915062Z"
    },
    "papermill": {
     "duration": 249.025965,
     "end_time": "2024-06-04T08:22:26.918017",
     "exception": false,
     "start_time": "2024-06-04T08:18:17.892052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 03:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=2.8292452494303384, metrics={'train_runtime': 248.6765, 'train_samples_per_second': 1.086, 'train_steps_per_second': 0.06, 'total_flos': 1265135863726080.0, 'train_loss': 2.8292452494303384, 'epoch': 2.608695652173913})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,    \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84197935",
   "metadata": {
    "papermill": {
     "duration": 0.013863,
     "end_time": "2024-06-04T08:22:26.946599",
     "exception": false,
     "start_time": "2024-06-04T08:22:26.932736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26c38407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:22:26.976430Z",
     "iopub.status.busy": "2024-06-04T08:22:26.975725Z",
     "iopub.status.idle": "2024-06-04T08:22:28.251110Z",
     "shell.execute_reply": "2024-06-04T08:22:28.250143Z"
    },
    "papermill": {
     "duration": 1.292761,
     "end_time": "2024-06-04T08:22:28.253144",
     "exception": false,
     "start_time": "2024-06-04T08:22:26.960383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.010437        0.723144    0.266420\n",
      "1   211333        0.007062        0.017718    0.975220\n",
      "2  1233961        0.878157        0.000179    0.121664\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds_test = predictions.predictions\n",
    "\n",
    "# Since preds_test contains logits, apply softmax to get probabilities\n",
    "import torch.nn.functional as F\n",
    "preds_test_probabilities = F.softmax(torch.tensor(preds_test), dim=-1).numpy()\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test[\"id\"],\n",
    "    'winner_model_a': preds_test_probabilities[:, 0],\n",
    "    'winner_model_b': preds_test_probabilities[:, 1], \n",
    "    'winner_tie': preds_test_probabilities[:, 2]\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Print the first few rows of the submission DataFrame\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285527da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:22:28.283724Z",
     "iopub.status.busy": "2024-06-04T08:22:28.282963Z",
     "iopub.status.idle": "2024-06-04T08:22:28.513467Z",
     "shell.execute_reply": "2024-06-04T08:22:28.512487Z"
    },
    "papermill": {
     "duration": 0.247904,
     "end_time": "2024-06-04T08:22:28.515595",
     "exception": false,
     "start_time": "2024-06-04T08:22:28.267691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lmsys/tokenizer/tokenizer_config.json',\n",
       " 'lmsys/tokenizer/special_tokens_map.json',\n",
       " 'lmsys/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(os.path.join(output_dir, \"tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce6aef6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T08:22:28.547990Z",
     "iopub.status.busy": "2024-06-04T08:22:28.547700Z",
     "iopub.status.idle": "2024-06-04T08:22:28.603569Z",
     "shell.execute_reply": "2024-06-04T08:22:28.602609Z"
    },
    "papermill": {
     "duration": 0.073832,
     "end_time": "2024-06-04T08:22:28.605583",
     "exception": false,
     "start_time": "2024-06-04T08:22:28.531751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join(output_dir, \"model\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 414.302768,
   "end_time": "2024-06-04T08:22:31.464907",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-04T08:15:37.162139",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "194c4e36d8e0452e9d06ac5889f1cf12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a27395b54084391aaacdcdb810b8afc",
       "placeholder": "​",
       "style": "IPY_MODEL_7698953c880d4e0ca3ad216fe32dd7b5",
       "value": " 4/4 [01:54&lt;00:00, 22.59s/it]"
      }
     },
     "1fac6df8f92340628d8256882e0297b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a86bb1e4e724a9d8a5bf8f07285cb95",
       "placeholder": "​",
       "style": "IPY_MODEL_dfc515334dde4ccda35e31e80b1880de",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "3a86bb1e4e724a9d8a5bf8f07285cb95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d79476e0f184b9594e3fcfa86af0f74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "429145a4cc2744238ad41fc7b5ab1864": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3d79476e0f184b9594e3fcfa86af0f74",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b9060a0b4105441eb2d86739b5b855da",
       "value": 4.0
      }
     },
     "52d68b5504d842619170dcfe109df345": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1fac6df8f92340628d8256882e0297b9",
        "IPY_MODEL_429145a4cc2744238ad41fc7b5ab1864",
        "IPY_MODEL_194c4e36d8e0452e9d06ac5889f1cf12"
       ],
       "layout": "IPY_MODEL_8edd1dafbb41440db344694a92ce8c69"
      }
     },
     "7698953c880d4e0ca3ad216fe32dd7b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8edd1dafbb41440db344694a92ce8c69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a27395b54084391aaacdcdb810b8afc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9060a0b4105441eb2d86739b5b855da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dfc515334dde4ccda35e31e80b1880de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
